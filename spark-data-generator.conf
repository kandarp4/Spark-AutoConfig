spark-bench = {
  spark-submit-config = [{
    conf = {
      // Any configuration you need for your setup goes here, like:
      // "spark.dynamicAllocation.enabled" = "false"
      "spark.eventLog.enabled" = "true",
      "spark.eventLog.dir" = "s3://can300-ttc-adhoc-api/spark-autoconfig/even-logs/"
    }
    suites-parallel = false
    workload-suites = [
      {
        descr = "Generate a dataset, then take that same dataset and write it out to Parquet format"
        benchmark-output = "hdfs:///tmp/csv-vs-parquet/results-data-gen.csv"
        // We need to generate the dataset first through the data generator, then we take that dataset and convert it to Parquet.
        parallel = false
        workloads = [
          {
            name = "data-generation-kmeans"
            rows = 10000000
            cols = 24
            output = "s3://can300-ttc-adhoc-api/spark-autoconfig/input_data/kmeans-data.parquet"
          },
          {
              name = "graph-data-generator"
              vertices = 1000
              output = "s3://can300-ttc-adhoc-api/spark-autoconfig/input_data/one-thousand-vertex-graph.txt"
          },
          {
            name = "data-generation-kmeans"
            rows = 100000000
            cols = 24
            output = "s3://can300-ttc-adhoc-api/spark-autoconfig/input_data/kmeans-data.parquet"
            k = 4500
            scaling = 1.6
            parititions = 10
          }
        ]
      }
    ]
  }]
}
